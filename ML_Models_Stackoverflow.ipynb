{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://www.sqlitetutorial.net/sqlite-python/create-tables/\n",
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to the SQLite database\n",
    "        specified by db_file\n",
    "    :param db_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    " \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking 0.5 Million entries to a dataframe.\n",
    "write_db = 'Titlemoreweight.db'\n",
    "if os.path.isfile(write_db):\n",
    "    conn_r = create_connection(write_db)\n",
    "    if conn_r is not None:\n",
    "        preprocessed_data = pd.read_sql_query(\"\"\"SELECT question, Tags FROM QuestionsProcessed limit 500000\"\"\", conn_r)\n",
    "conn_r.commit()\n",
    "conn_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dynam datagrid bind silverlight dynam datagrid...</td>\n",
       "      <td>c# silverlight data-binding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dynam datagrid bind silverlight dynam datagrid...</td>\n",
       "      <td>c# silverlight data-binding columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>java.lang.noclassdeffounderror javax servlet j...</td>\n",
       "      <td>jsp jstl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>java.sql.sqlexcept microsoft odbc driver manag...</td>\n",
       "      <td>java jdbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>better way updat feed fb php sdk better way up...</td>\n",
       "      <td>facebook api facebook-php-sdk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  dynam datagrid bind silverlight dynam datagrid...   \n",
       "1  dynam datagrid bind silverlight dynam datagrid...   \n",
       "2  java.lang.noclassdeffounderror javax servlet j...   \n",
       "3  java.sql.sqlexcept microsoft odbc driver manag...   \n",
       "4  better way updat feed fb php sdk better way up...   \n",
       "\n",
       "                                  tags  \n",
       "0          c# silverlight data-binding  \n",
       "1  c# silverlight data-binding columns  \n",
       "2                             jsp jstl  \n",
       "3                            java jdbc  \n",
       "4        facebook api facebook-php-sdk  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data points in sample : 500000\n",
      "number of dimensions : 2\n"
     ]
    }
   ],
   "source": [
    "print(\"number of data points in sample :\", preprocessed_data.shape[0])\n",
    "print(\"number of dimensions :\", preprocessed_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Converting string Tags to multilable output variables __ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\n",
    "multilabel_y = vectorizer.fit_transform(preprocessed_data['tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pbtD0Hx8U99c"
   },
   "source": [
    "__ Selecting 500 Tags __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_to_choose(n):\n",
    "    t = multilabel_y.sum(axis=0).tolist()[0]\n",
    "    sorted_tags_i = sorted(range(len(t)), key=lambda i: t[i], reverse=True)\n",
    "    multilabel_yn=multilabel_y[:,sorted_tags_i[:n]]\n",
    "    return multilabel_yn\n",
    "\n",
    "def questions_explained_fn(n):\n",
    "    multilabel_yn = tags_to_choose(n)\n",
    "    x= multilabel_yn.sum(axis=1)\n",
    "    return (np.count_nonzero(x==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h_nMDxAIU99d"
   },
   "outputs": [],
   "source": [
    "questions_explained = []\n",
    "total_tags=multilabel_y.shape[1]\n",
    "total_qs=preprocessed_data.shape[0]\n",
    "for i in range(500, total_tags, 100):\n",
    "    questions_explained.append(np.round(((total_qs-questions_explained_fn(i))/total_qs)*100,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of questions that are not covered : 45221 out of  500000\n"
     ]
    }
   ],
   "source": [
    "# we will be taking 500 tags\n",
    "multilabel_yx = tags_to_choose(500)\n",
    "print(\"number of questions that are not covered :\", questions_explained_fn(500),\"out of \", total_qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4.2 Split the data into test and train (80:20) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasize=400000\n",
    "x_train=preprocessed_data.head(train_datasize)\n",
    "x_test=preprocessed_data.tail(preprocessed_data.shape[0] - train_datasize) #400000\n",
    "\n",
    "y_train = multilabel_yx[0:train_datasize,:]\n",
    "y_test = multilabel_yx[train_datasize:preprocessed_data.shape[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points in train data : (400000, 500)\n",
      "Number of data points in test data : (100000, 500)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data points in train data :\", y_train.shape)\n",
    "print(\"Number of data points in test data :\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Featurizing data with TfIdf vectorizer </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 0:03:52.648371\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "vectorizer = TfidfVectorizer(min_df=0.00009, max_features=200000, smooth_idf=True, norm=\"l2\", \\\n",
    "                             tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(1,3))\n",
    "x_train_tfidf = vectorizer.fit_transform(x_train['question'])\n",
    "x_test_tfidf = vectorizer.transform(x_test['question'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of train data X: (400000, 94927) Y : (400000, 500)\n",
      "Dimensions of test data X: (100000, 94927) Y: (100000, 500)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensions of train data X:\",x_train_tfidf.shape, \"Y :\",y_train.shape)\n",
    "print(\"Dimensions of test data X:\",x_test_tfidf.shape,\"Y:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Featurizing data with BoW vectorizer </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 0:04:44.483483\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "vectorizer = CountVectorizer(min_df=0.00009, max_features=200000, ngram_range=(1,3),binary=True,tokenizer = lambda x: x.split())\n",
    "vectorizer.fit(x_train['question'])\n",
    "x_train_bow = vectorizer.transform(x_train['question'])\n",
    "x_test_bow = vectorizer.transform(x_test['question'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of train data X: (400000, 94927) Y : (400000, 500)\n",
      "Dimensions of test data X: (100000, 94927) Y: (100000, 500)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensions of train data X:\",x_train_bow.shape, \"Y :\",y_train.shape)\n",
    "print(\"Dimensions of test data X:\",x_test_bow.shape,\"Y:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Logistic Regression with HyperParameter Tuning on BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 8/8 [11:31:56<00:00, 7752.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1e-05: 0.0, 0.0001: 0.03133729148996325, 0.001: 0.17032792430781216, 0.01: 0.3368799826286565, 0.1: 0.43849109024341437, 1: 0.4596690189404498, 10: 0.4549609431176056, 100: 0.4512246172427754}\n"
     ]
    }
   ],
   "source": [
    "# Please write all the code with proper documentation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "summary=[]\n",
    "f1_test_bow_dict={}\n",
    "\n",
    "\n",
    "alpha=[0.00001,0.0001,0.001,0.01,0.1,1,10,100]\n",
    "for i in tqdm(alpha):\n",
    "    # create instance of model\n",
    "    lr=OneVsRestClassifier(LogisticRegression(penalty='l2', C=i), n_jobs=-1)\n",
    "    \n",
    "     # fitting the model on crossvalidation train\n",
    "    lr.fit(x_train_bow, y_train)\n",
    "    \n",
    "    # predict the response on the crossvalidation train\n",
    "    pred = lr.predict(x_test_bow)\n",
    "        \n",
    "    f1 = f1_score(y_test, pred, average='micro')\n",
    "    \n",
    "    f1_test_bow_dict[i]=f1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(f1_test_bow_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best alpha value from the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n",
    "def find_highest_alpha(k_dict):\n",
    "    k=max(k_dict, key=k_dict.get)\n",
    "    return k \n",
    "\n",
    "print(find_highest_alpha(f1_test_bow_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,accuracy_score,hamming_loss\n",
    "\n",
    "\n",
    "lr=OneVsRestClassifier(LogisticRegression(penalty='l2', C=find_highest_alpha(f1_test_bow_dict)), n_jobs=-1)\n",
    "lr.fit(x_train_bow, y_train)\n",
    "pred = lr.predict(x_test_bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_bow=accuracy_score(y_test, pred)\n",
    "hamming_bow=hamming_loss(y_test,pred)\n",
    "\n",
    "f1_score_bow_micro=f1_score(y_test, pred, average='micro')\n",
    "precision_bow_micro=precision_score(y_test, pred, average='micro')\n",
    "recall_bow_micro=recall_score(y_test, pred, average='micro')\n",
    "\n",
    "f1_score_bow_macro=f1_score(y_test, pred, average='macro')\n",
    "precision_bow_macro=precision_score(y_test, pred, average='macro')\n",
    "recall_bow_macro=recall_score(y_test, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.append(['BOW','Logistic Regression',find_highest_alpha(f1_test_bow_dict),accuracy_bow,hamming_bow,f1_score_bow_micro,precision_bow_micro,recall_bow_micro,f1_score_bow_macro,precision_bow_macro,recall_bow_macro])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Logistic Regression with HyperParameter Tuning on TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 8/8 [6:28:42<00:00, 4611.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1e-05: 0.0, 0.0001: 0.0, 0.001: 6.903772911896351e-05, 0.01: 0.04054684819957372, 0.1: 0.2584937108746632, 1: 0.44464504481268946, 10: 0.4842800906237535, 100: 0.47650294455176984}\n"
     ]
    }
   ],
   "source": [
    "f1_test_tfidf_dict={}\n",
    "\n",
    "\n",
    "alpha=[0.00001,0.0001,0.001,0.01,0.1,1,10,100]\n",
    "for i in tqdm(alpha):\n",
    "    # create instance of model\n",
    "    lr=OneVsRestClassifier(LogisticRegression(penalty='l2', C=i), n_jobs=-1)\n",
    "    \n",
    "     # fitting the model on crossvalidation train\n",
    "    lr.fit(x_train_tfidf, y_train)\n",
    "    \n",
    "    # predict the response on the crossvalidation train\n",
    "    pred = lr.predict(x_test_tfidf)\n",
    "        \n",
    "    f1 = f1_score(y_test, pred, average='micro')\n",
    "    \n",
    "    f1_test_tfidf_dict[i]=f1\n",
    "    \n",
    "print(f1_test_tfidf_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(find_highest_alpha(f1_test_tfidf_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=OneVsRestClassifier(LogisticRegression(penalty='l2', C=find_highest_alpha(f1_test_tfidf_dict)), n_jobs=-1)\n",
    "lr.fit(x_train_tfidf, y_train)\n",
    "pred = lr.predict(x_test_tfidf)\n",
    "\n",
    "accuracy_tfidf=accuracy_score(y_test, pred)\n",
    "hamming_tfidf=hamming_loss(y_test,pred)\n",
    "\n",
    "f1_score_tfidf_micro=f1_score(y_test, pred, average='micro')\n",
    "precision_tfidf_micro=precision_score(y_test, pred, average='micro')\n",
    "recall_tfidf_micro=recall_score(y_test, pred, average='micro')\n",
    "\n",
    "f1_score_tfidf_macro=f1_score(y_test, pred, average='macro')\n",
    "precision_tfidf_macro=precision_score(y_test, pred, average='macro')\n",
    "recall_tfidf_macro=recall_score(y_test, pred, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.append(['TFIDF','Logistic Regression',find_highest_alpha(f1_test_tfidf_dict),accuracy_tfidf,hamming_tfidf,f1_score_tfidf_micro,precision_tfidf_micro,recall_tfidf_micro,f1_score_tfidf_macro,precision_tfidf_macro,recall_tfidf_macro])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Linear SVM with HyperParameter Tuning on BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 8/8 [1:42:13<00:00, 811.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1e-05: 0.4195889729327029, 0.0001: 0.4342847526857868, 0.001: 0.32237153381177897, 0.01: 0.1633757591974595, 0.1: 0.0353479244091291, 1: 0.0, 10: 0.0, 100: 0.03419900912758285}\n"
     ]
    }
   ],
   "source": [
    "# Please write all the code with proper documentation\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "f1_test_bow_dict={}\n",
    "\n",
    "\n",
    "alpha=[0.00001,0.0001,0.001,0.01,0.1,1,10,100]\n",
    "for i in tqdm(alpha):\n",
    "    # create instance of model\n",
    "    sgd=OneVsRestClassifier(SGDClassifier(loss='hinge',penalty='l1', alpha=i), n_jobs=-1)\n",
    "    \n",
    "     # fitting the model on crossvalidation train\n",
    "    sgd.fit(x_train_bow, y_train)\n",
    "    \n",
    "    # predict the response on the crossvalidation train\n",
    "    pred = sgd.predict(x_test_bow)\n",
    "        \n",
    "    f1 = f1_score(y_test, pred, average='micro')\n",
    "    \n",
    "    f1_test_bow_dict[i]=f1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(f1_test_bow_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best alpha value from the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n",
    "def find_highest_alpha(k_dict):\n",
    "    k=max(k_dict, key=k_dict.get)\n",
    "    return k \n",
    "\n",
    "print(find_highest_alpha(f1_test_bow_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,accuracy_score,hamming_loss\n",
    "\n",
    "\n",
    "sgd=OneVsRestClassifier(SGDClassifier(loss='hinge',penalty='l1', alpha=find_highest_alpha(f1_test_bow_dict)), n_jobs=-1)\n",
    "sgd.fit(x_train_bow, y_train)\n",
    "pred = sgd.predict(x_test_bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_bow=accuracy_score(y_test, pred)\n",
    "hamming_bow=hamming_loss(y_test,pred)\n",
    "\n",
    "f1_score_bow_micro=f1_score(y_test, pred, average='micro')\n",
    "precision_bow_micro=precision_score(y_test, pred, average='micro')\n",
    "recall_bow_micro=recall_score(y_test, pred, average='micro')\n",
    "\n",
    "f1_score_bow_macro=f1_score(y_test, pred, average='macro')\n",
    "precision_bow_macro=precision_score(y_test, pred, average='macro')\n",
    "recall_bow_macro=recall_score(y_test, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.append(['BOW','Linear SVM',find_highest_alpha(f1_test_bow_dict),accuracy_bow,hamming_bow,f1_score_bow_micro,precision_bow_micro,recall_bow_micro,f1_score_bow_macro,precision_bow_macro,recall_bow_macro])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Linear SVM with HyperParameter Tuning on TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 8/8 [1:27:00<00:00, 695.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1e-05: 0.4330509258869754, 0.0001: 0.33602794593704854, 0.001: 0.16853310711902633, 0.01: 0.011676644994539081, 0.1: 0.0, 1: 0.0, 10: 0.0, 100: 0.01404449295367725}\n"
     ]
    }
   ],
   "source": [
    "f1_test_tfidf_dict={}\n",
    "\n",
    "\n",
    "alpha=[0.00001,0.0001,0.001,0.01,0.1,1,10,100]\n",
    "for i in tqdm(alpha):\n",
    "    # create instance of model\n",
    "    sgd=OneVsRestClassifier(SGDClassifier(loss='hinge',penalty='l1', alpha=i), n_jobs=-1)\n",
    "    \n",
    "     # fitting the model on crossvalidation train\n",
    "    sgd.fit(x_train_tfidf, y_train)\n",
    "    \n",
    "    # predict the response on the crossvalidation train\n",
    "    pred = sgd.predict(x_test_tfidf)\n",
    "        \n",
    "    f1 = f1_score(y_test, pred, average='micro')\n",
    "    \n",
    "    f1_test_tfidf_dict[i]=f1\n",
    "    \n",
    "print(f1_test_tfidf_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05\n"
     ]
    }
   ],
   "source": [
    "print(find_highest_alpha(f1_test_tfidf_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd=OneVsRestClassifier(SGDClassifier(loss='hinge',penalty='l1', alpha=find_highest_alpha(f1_test_tfidf_dict)), n_jobs=-1)\n",
    "sgd.fit(x_train_tfidf, y_train)\n",
    "pred = sgd.predict(x_test_tfidf)\n",
    "\n",
    "accuracy_tfidf=accuracy_score(y_test, pred)\n",
    "hamming_tfidf=hamming_loss(y_test,pred)\n",
    "\n",
    "f1_score_tfidf_micro=f1_score(y_test, pred, average='micro')\n",
    "precision_tfidf_micro=precision_score(y_test, pred, average='micro')\n",
    "recall_tfidf_micro=recall_score(y_test, pred, average='micro')\n",
    "\n",
    "f1_score_tfidf_macro=f1_score(y_test, pred, average='macro')\n",
    "precision_tfidf_macro=precision_score(y_test, pred, average='macro')\n",
    "recall_tfidf_macro=recall_score(y_test, pred, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.append(['TFIDF','Linear SVM',find_highest_alpha(f1_test_tfidf_dict),accuracy_tfidf,hamming_tfidf,f1_score_tfidf_micro,precision_tfidf_micro,recall_tfidf_micro,f1_score_tfidf_macro,precision_tfidf_macro,recall_tfidf_macro])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------+----------------+----------+------------+----------------------+-----------------------+--------------------+----------------------+-----------------------+---------------------+\n",
      "| Vectorizer |        Model        | HyperParameter | Accuracy |  ham_loss  | f1_score_tfidf_micro | precision_tfidf_micro | recall_tfidf_micro | f1_score_tfidf_macro | precision_tfidf_macro |  recall_tfidf_macro |\n",
      "+------------+---------------------+----------------+----------+------------+----------------------+-----------------------+--------------------+----------------------+-----------------------+---------------------+\n",
      "|    BOW     | Logistic Regression |       1        | 0.22311  | 0.00294636 |  0.4596690189404498  |   0.6340355350493767  |  0.36052171311532  |  0.3173558544567375  |  0.48177142093047165  |  0.2551698621845632 |\n",
      "|   TFIDF    | Logistic Regression |       10       | 0.24792  | 0.00274068 |  0.4842800906237535  |   0.7000935779417206  | 0.3701700688099786 | 0.36712935046571693  |   0.5620502409404423  |  0.2871254800518158 |\n",
      "|    BOW     |      Linear SVM     |     0.0001     | 0.21414  | 0.00297258 |  0.4317529257484984  |   0.643501054191122   | 0.3248567417669666 |  0.2578728610570023  |   0.3506748241683588  | 0.23805703325067804 |\n",
      "|   TFIDF    |      Linear SVM     |     1e-05      | 0.24785  | 0.00269558 |  0.4320240036747198  |   0.8074190753721352  | 0.2949105930545647 |  0.2642373622906472  |   0.4252775242728761  | 0.21781329904238456 |\n",
      "+------------+---------------------+----------------+----------+------------+----------------------+-----------------------+--------------------+----------------------+-----------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "# Please compare all your models using Prettytable library\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Vectorizer\", \"Model\", \"HyperParameter\",\"Accuracy\",\"ham_loss\",\"f1_score_tfidf_micro\",\"precision_tfidf_micro\",\"recall_tfidf_micro\",\"f1_score_tfidf_macro\",\"precision_tfidf_macro\",\"recall_tfidf_macro\"]\n",
    "\n",
    "for each in summary:\n",
    "    x.add_row(each)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"Vectorizer\", \"Model\", \"HyperParameter\",\"Accuracy\",\"ham_loss\",\"f1_score_tfidf_micro\",\"precision_tfidf_micro\",\"recall_tfidf_micro\",\"f1_score_tfidf_macro\",\"precision_tfidf_macro\",\"recall_tfidf_macro\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Model</th>\n",
       "      <th>HyperParameter</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ham_loss</th>\n",
       "      <th>f1_score_tfidf_micro</th>\n",
       "      <th>precision_tfidf_micro</th>\n",
       "      <th>recall_tfidf_micro</th>\n",
       "      <th>f1_score_tfidf_macro</th>\n",
       "      <th>precision_tfidf_macro</th>\n",
       "      <th>recall_tfidf_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BOW</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.22311</td>\n",
       "      <td>0.002946</td>\n",
       "      <td>0.459669</td>\n",
       "      <td>0.634036</td>\n",
       "      <td>0.360522</td>\n",
       "      <td>0.317356</td>\n",
       "      <td>0.481771</td>\n",
       "      <td>0.255170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TFIDF</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>0.24792</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>0.484280</td>\n",
       "      <td>0.700094</td>\n",
       "      <td>0.370170</td>\n",
       "      <td>0.367129</td>\n",
       "      <td>0.562050</td>\n",
       "      <td>0.287125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BOW</td>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.21414</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>0.431753</td>\n",
       "      <td>0.643501</td>\n",
       "      <td>0.324857</td>\n",
       "      <td>0.257873</td>\n",
       "      <td>0.350675</td>\n",
       "      <td>0.238057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TFIDF</td>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.24785</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.432024</td>\n",
       "      <td>0.807419</td>\n",
       "      <td>0.294911</td>\n",
       "      <td>0.264237</td>\n",
       "      <td>0.425278</td>\n",
       "      <td>0.217813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Vectorizer                Model  HyperParameter  Accuracy  ham_loss  \\\n",
       "0        BOW  Logistic Regression         1.00000   0.22311  0.002946   \n",
       "1      TFIDF  Logistic Regression        10.00000   0.24792  0.002741   \n",
       "2        BOW           Linear SVM         0.00010   0.21414  0.002973   \n",
       "3      TFIDF           Linear SVM         0.00001   0.24785  0.002696   \n",
       "\n",
       "   f1_score_tfidf_micro  precision_tfidf_micro  recall_tfidf_micro  \\\n",
       "0              0.459669               0.634036            0.360522   \n",
       "1              0.484280               0.700094            0.370170   \n",
       "2              0.431753               0.643501            0.324857   \n",
       "3              0.432024               0.807419            0.294911   \n",
       "\n",
       "   f1_score_tfidf_macro  precision_tfidf_macro  recall_tfidf_macro  \n",
       "0              0.317356               0.481771            0.255170  \n",
       "1              0.367129               0.562050            0.287125  \n",
       "2              0.257873               0.350675            0.238057  \n",
       "3              0.264237               0.425278            0.217813  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
